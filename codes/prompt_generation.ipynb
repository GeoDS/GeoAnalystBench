{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Open Source</th>\n",
       "      <th>Task Categories1</th>\n",
       "      <th>Task Categories2</th>\n",
       "      <th>Task Categories3</th>\n",
       "      <th>Task</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Domain Knowledge</th>\n",
       "      <th>Dataset Description</th>\n",
       "      <th>Human Designed Workflow</th>\n",
       "      <th>Task Length</th>\n",
       "      <th>CodeString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "      <td>Making predictions</td>\n",
       "      <td>Understanding where</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Find heat islands and at-risk populations in M...</td>\n",
       "      <td>Your task is analyzing urban heat using Krigin...</td>\n",
       "      <td>Kriging is a commonly used spatial interpolati...</td>\n",
       "      <td>dataset/Temperature.geojson: Geojson file that...</td>\n",
       "      <td>1. Load dataset\\n2. Interpolate(Temperature)\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>import numpy as np\\nimport geopandas as gpd\\nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>Detecting and quantifying patterns</td>\n",
       "      <td>Finding the best locations and paths</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Find future bus stop locations in Hamilton, Te...</td>\n",
       "      <td>Your task is performing analysis on public tra...</td>\n",
       "      <td>The Overlay toolset contains tools to overlay ...</td>\n",
       "      <td>dataset/BusServiceArea.geojson: Geojson file s...</td>\n",
       "      <td>1. Load dataset\\n2. Filter(Poverty)\\n3. Filter...</td>\n",
       "      <td>6</td>\n",
       "      <td>import geopandas as gpd\\nimport matplotlib.pyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>Understanding where</td>\n",
       "      <td>Detecting and quantifying patterns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assess burn scars and understanding the impact...</td>\n",
       "      <td>Your task is assessing burn scars using satell...</td>\n",
       "      <td>Normalized Burn Ratio (NBR) is used to identif...</td>\n",
       "      <td>dataset/G_2014.tif: Raster file of satellite i...</td>\n",
       "      <td>1. Load dataset\\n2. Filter(2014 bands)\\n3. Fil...</td>\n",
       "      <td>6</td>\n",
       "      <td>import rasterio\\nimport numpy as np\\nimport ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>Understanding where</td>\n",
       "      <td>Finding the best locations and paths</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Identify groundwater vulnerable areas that nee...</td>\n",
       "      <td>Your task is identifying groundwater vulnerabl...</td>\n",
       "      <td>Suitability modeling is an analytical process ...</td>\n",
       "      <td>dataset/mc_soils.shp: In this shapefile, three...</td>\n",
       "      <td>1. Load dataset\\n2. Project (shapefile)\\n3. Po...</td>\n",
       "      <td>10</td>\n",
       "      <td>import arcpy\\nfrom arcpy.sa import *\\nfrom arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>Detecting and quantifying patterns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visualize the data about children with elevate...</td>\n",
       "      <td>Your task is visualizing data about children w...</td>\n",
       "      <td>Hot spot analysis is based on mathematical cal...</td>\n",
       "      <td>High_Blood_Level_Results.shp: This shapefile c...</td>\n",
       "      <td>1.Load dataset\\n2. Perform Optimized Hot Spot ...</td>\n",
       "      <td>5</td>\n",
       "      <td>import arcpy\\n\\n# Set up the input shapefiles ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id Open Source                    Task Categories1  \\\n",
       "0   1           T                  Making predictions   \n",
       "1   2           T  Detecting and quantifying patterns   \n",
       "2   3           T                 Understanding where   \n",
       "3   4           F                 Understanding where   \n",
       "4   5           F  Detecting and quantifying patterns   \n",
       "\n",
       "                       Task Categories2 Task Categories3  \\\n",
       "0                   Understanding where              NaN   \n",
       "1  Finding the best locations and paths              NaN   \n",
       "2    Detecting and quantifying patterns              NaN   \n",
       "3  Finding the best locations and paths              NaN   \n",
       "4                                   NaN              NaN   \n",
       "\n",
       "                                                Task  \\\n",
       "0  Find heat islands and at-risk populations in M...   \n",
       "1  Find future bus stop locations in Hamilton, Te...   \n",
       "2  Assess burn scars and understanding the impact...   \n",
       "3  Identify groundwater vulnerable areas that nee...   \n",
       "4  Visualize the data about children with elevate...   \n",
       "\n",
       "                                         Instruction  \\\n",
       "0  Your task is analyzing urban heat using Krigin...   \n",
       "1  Your task is performing analysis on public tra...   \n",
       "2  Your task is assessing burn scars using satell...   \n",
       "3  Your task is identifying groundwater vulnerabl...   \n",
       "4  Your task is visualizing data about children w...   \n",
       "\n",
       "                                    Domain Knowledge  \\\n",
       "0  Kriging is a commonly used spatial interpolati...   \n",
       "1  The Overlay toolset contains tools to overlay ...   \n",
       "2  Normalized Burn Ratio (NBR) is used to identif...   \n",
       "3  Suitability modeling is an analytical process ...   \n",
       "4  Hot spot analysis is based on mathematical cal...   \n",
       "\n",
       "                                 Dataset Description  \\\n",
       "0  dataset/Temperature.geojson: Geojson file that...   \n",
       "1  dataset/BusServiceArea.geojson: Geojson file s...   \n",
       "2  dataset/G_2014.tif: Raster file of satellite i...   \n",
       "3  dataset/mc_soils.shp: In this shapefile, three...   \n",
       "4  High_Blood_Level_Results.shp: This shapefile c...   \n",
       "\n",
       "                             Human Designed Workflow  Task Length  \\\n",
       "0  1. Load dataset\\n2. Interpolate(Temperature)\\n...            7   \n",
       "1  1. Load dataset\\n2. Filter(Poverty)\\n3. Filter...            6   \n",
       "2  1. Load dataset\\n2. Filter(2014 bands)\\n3. Fil...            6   \n",
       "3  1. Load dataset\\n2. Project (shapefile)\\n3. Po...           10   \n",
       "4  1.Load dataset\\n2. Perform Optimized Hot Spot ...            5   \n",
       "\n",
       "                                          CodeString  \n",
       "0  import numpy as np\\nimport geopandas as gpd\\nf...  \n",
       "1  import geopandas as gpd\\nimport matplotlib.pyp...  \n",
       "2  import rasterio\\nimport numpy as np\\nimport ma...  \n",
       "3  import arcpy\\nfrom arcpy.sa import *\\nfrom arc...  \n",
       "4  import arcpy\\n\\n# Set up the input shapefiles ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"GeoAnalystBench.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                         1\n",
       "Open Source                                                                T\n",
       "Task Categories1                                          Making predictions\n",
       "Task Categories2                                         Understanding where\n",
       "Task Categories3                                                         NaN\n",
       "Task                       Find heat islands and at-risk populations in M...\n",
       "Instruction                Your task is analyzing urban heat using Krigin...\n",
       "Domain Knowledge           Kriging is a commonly used spatial interpolati...\n",
       "Dataset Description        dataset/Temperature.geojson: Geojson file that...\n",
       "Human Designed Workflow    1. Load dataset\\n2. Interpolate(Temperature)\\n...\n",
       "Task Length                                                                7\n",
       "CodeString                 import numpy as np\\nimport geopandas as gpd\\nf...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the first task as an example\n",
    "ID = 1 # Use your own task id\n",
    "\n",
    "row = data.iloc[ID-1]\n",
    "if row['Open Source'] == 'T':\n",
    "  Arcpy = False\n",
    "else:\n",
    "  Arcpy = True\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line_breaks(long_string, char_limit=80): #limit char from each line to 80\n",
    "  words = long_string.split()\n",
    "  new_string = \"\"\n",
    "  char_count = 0\n",
    "  for word in words:\n",
    "    new_string += word + \" \"\n",
    "    char_count += len(word)\n",
    "    if char_count > char_limit:\n",
    "      new_string += \"\\n\"\n",
    "      char_count = 0\n",
    "  return new_string\n",
    "\n",
    "def long_line_break(long_string): #limit string for being too long per line\n",
    "  result = \"\"\n",
    "\n",
    "  if isinstance(long_string, str):\n",
    "    for line in long_string.split(\"\\n\"):\n",
    "      new_line = add_line_breaks(line)\n",
    "      result += new_line + \"\\n\"\n",
    "  else:\n",
    "    result = str(long_string)\n",
    "  return result\n",
    "\n",
    "\n",
    "task = row[\"Task\"]\n",
    "instruction = row[\"Instruction\"]\n",
    "domainKnowledge = row[\"Domain Knowledge\"]\n",
    "dataset = row[\"Dataset Description\"]\n",
    "\n",
    "instruction = add_line_breaks(instruction)\n",
    "task = add_line_breaks(task)\n",
    "domainKnowledge = add_line_breaks(domainKnowledge)\n",
    "dataset = long_line_break(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflowTemplate(IDs=None, tasks=None, instructions=None, zeroShot=False, domainKnowledges=None, datasets=None):\n",
    "  if task is None and instruction is None:\n",
    "    print(\"Task or Instruction is necessary\")\n",
    "    return None\n",
    "  prompt = {\n",
    "    \"Task\": tasks,\n",
    "    \"Instruction\": instructions,\n",
    "    \"Domain Knowledge\": domainKnowledges,\n",
    "    \"Dataset Description\": datasets\n",
    "  }\n",
    "\n",
    "  template = \"\"\"As a Geospatial data scientist, you will generate a workflow to a proposed task.\\n\"\"\" #Define role\n",
    "  for key, value in prompt.items():\n",
    "    if value is not None:\n",
    "      template += f\"\\n[{key}]: \\n{value}\" #include information\n",
    "\n",
    "  #one shot sample\n",
    "  sample = \"\"\" \\n\\\"\\\"\\\"\n",
    "  tasks = [“task1”, “task2”, “task3”]\n",
    "\n",
    "  G = nx.DiGraph()\n",
    "  for i in range(len(tasks) - 1):\n",
    "      G.add_edge(tasks[i], tasks[i + 1])\n",
    "  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=\"dot\")\n",
    "  plt.figure(figsize=(15, 8))\n",
    "  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)\n",
    "  plt.title(\"Workflow for Analyzing Urban Heat Using Kriging Interpolation\", fontsize=14)\n",
    "  plt.show()\\n\\\"\\\"\\\"\n",
    "  \"\"\"\n",
    "\n",
    "  #Key Notes\n",
    "  template += '\\n[Key Notes]:'\n",
    "  template += '\\n1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).'\n",
    "  template += '\\n2.Using **NetworkX* package for visualization.'\n",
    "  template += '\\n3.Using \\'dot\\' for graph visualization layout.'\n",
    "  template += '\\n4.Multiple subtasks can be proceeded correspondingly because'\n",
    "  template += '\\nall of their outputs will be inputs for the next subtask.'\n",
    "  template += \"\\n5.Limiting your output to code, no extra information.\"\n",
    "  template += '\\n6.Only codes for workflow, no implementation.'\n",
    "  template += '\\n'\n",
    "  if zeroShot is False:\n",
    "    template += \"\\n[Expected Sample Output Begin]\"\n",
    "    template += \"\\n\" + sample\n",
    "    template += \"[Expected Sample Output End]\"\n",
    "  return template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeTemplate(IDs=None, tasks=None, instructions=None, zeroShot=False, domainKnowledges=None, datasets=None, Arcpy=False):\n",
    "  if task is None and instruction is None:\n",
    "    print(\"Task or Instruction is necessary\")\n",
    "    return None\n",
    "  prompt = {\n",
    "    \"Task\": tasks,\n",
    "    \"Instruction\": instructions,\n",
    "    \"Domain Knowledge\": domainKnowledges,\n",
    "    \"Dataset Description\": datasets\n",
    "  }\n",
    "\n",
    "  template = \"\"\"As a Geospatial data scientist, generate a python file to solve the proposed task.\\n\"\"\"\n",
    "  for key, value in prompt.items():\n",
    "    if value is not None:\n",
    "      template += f\"\\n[{key}]: \\n{value}\"\n",
    "\n",
    "  sample = \"\"\" \\\"\\\"\\\"\n",
    "    import packages\n",
    "\n",
    "    def main():\n",
    "      path = \"path\"\n",
    "      data = loaddata()\n",
    "      #code for subtask1\n",
    "      #code for subtask2\n",
    "      #code for final task\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "      main()\n",
    "  \\\"\\\"\\\"\n",
    "  \"\"\"\n",
    "  template += '\\n\\n[Key Notes]:'\n",
    "  template += '\\n1.Use **automatic reasoning** and clearly explain each subtask before performing it (ReAct approach).'\n",
    "  template += '\\n2.Using latest python packages for code generation'\n",
    "  template += \"\\n3.Put all code under main function, no helper functions\"\n",
    "  template += \"\\n4.Limit your output to code, no extra information.\"\n",
    "  if Arcpy is True:\n",
    "    template += \"\\n5.Use latest **Arcpy** functions only\"\n",
    "  else:\n",
    "    template += \"\\n5.Use latest open source python packages only\"\n",
    "  template += '\\n'\n",
    "  if zeroShot is False:\n",
    "    template += \"\\n[Expected Sample Output Begin]\"\n",
    "    template += \"\\n\" + sample\n",
    "    template += \"[Expected Sample Output End]\"\n",
    "  return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Geospatial data scientist, generate a python file to solve the proposed task.\n",
      "\n",
      "[Task]: \n",
      "Find heat islands and at-risk populations in Madison, Wisconsin \n",
      "[Instruction]: \n",
      "Your task is analyzing urban heat using Kriging interpolation techniques in Python. The analysis \n",
      "should focus on understanding spatial patterns of urban heat islands by using point temperature \n",
      "data and interpolating these values across a city. You will have to use a demographic layer to extract \n",
      "and enhance the data visualization on the elder group(Age>65). The goal is to load the temperature \n",
      "sample data, apply the Kriging method to predict temperature across the urban area, and generate \n",
      "a choropleth map showing the average interpolated temperature surface in each census block group. \n",
      "Highlighting the area with high interpolated area as well as high density of the elder population. \n",
      "The final output should be saved as \"pred_results/interpolated_urban_heat.png\". \n",
      "\n",
      "[Key Notes]:\n",
      "1.Use **automatic reasoning** and clearly explain each subtask before performing it (ReAct approach).\n",
      "2.Using latest python packages for code generation\n",
      "3.Put all code under main function, no helper functions\n",
      "4.Limit your output to code, no extra information.\n",
      "5.Use latest open source python packages only\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_code_prompt = codeTemplate(tasks=task, instructions=instruction, zeroShot=True, Arcpy=False)\n",
    "sample_workflow_prompt = workflowTemplate(tasks=task, instructions=instruction, zeroShot=False)\n",
    "\n",
    "print(sample_code_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate prompts for first 50 tasks\n",
    "with open('code_prompts.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['task_id', 'type', 'domain_knowledge', 'dataset', 'Arcpy', 'prompt_content'])\n",
    "\n",
    "with open('workflow_prompts.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['task_id', 'type', 'domain_knowledge', 'dataset', 'Arcpy', 'prompt_content'])\n",
    "\n",
    "for id in range(50):\n",
    "    row = data.iloc[id]\n",
    "    task = row[\"Task\"]\n",
    "    instruction = row[\"Instruction\"]\n",
    "    domainKnowledge = row[\"Domain Knowledge\"]\n",
    "    dataset = row[\"Dataset Description\"]\n",
    "    if row[\"Open Source\"] != 'T':\n",
    "        Arcpy = True\n",
    "    else:\n",
    "        Arcpy = False\n",
    "\n",
    "    instruction = add_line_breaks(instruction)\n",
    "    task = add_line_breaks(task)\n",
    "    domainKnowledge = add_line_breaks(domainKnowledge)\n",
    "    dataset = long_line_break(dataset)\n",
    "\n",
    "    # Generate prompts with different combinations of domain knowledge and dataset descriptions\n",
    "    combinations = [ #each task has 4 combinations of domain knowledge and dataset descriptions\n",
    "        (False, False),\n",
    "        (True, False),\n",
    "        (False, True),\n",
    "        (True, True)\n",
    "    ]\n",
    "\n",
    "    for domain, dataset_included in combinations:\n",
    "        # Build parameters for template functions\n",
    "        # Build params for code template\n",
    "        code_params = {\n",
    "            'tasks': task,\n",
    "            'instructions': instruction,\n",
    "            'zeroShot': True,\n",
    "            'Arcpy': Arcpy\n",
    "        }\n",
    "\n",
    "        # Build params for workflow template (without Arcpy)\n",
    "        workflow_params = {\n",
    "            'tasks': task,\n",
    "            'instructions': instruction,\n",
    "            'zeroShot': False\n",
    "        }\n",
    "\n",
    "        if domain:\n",
    "            code_params['domainKnowledges'] = domainKnowledge\n",
    "            workflow_params['domainKnowledges'] = domainKnowledge\n",
    "        if dataset_included:\n",
    "            code_params['datasets'] = dataset\n",
    "            workflow_params['datasets'] = dataset\n",
    "\n",
    "        # Generate code and workflow prompts\n",
    "        code_prompt = codeTemplate(**code_params)\n",
    "        workflow_prompt = workflowTemplate(**workflow_params)\n",
    "\n",
    "        # Write to CSV files\n",
    "        with open('code_prompts.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([id+1, 'code', domain, dataset_included, Arcpy, code_prompt])\n",
    "        with open('workflow_prompts.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([id+1, 'workflow', domain, dataset_included, Arcpy, workflow_prompt])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
